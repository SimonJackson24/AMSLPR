name: CI/CD Pipeline

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main, develop ]
  schedule:
    # Run daily performance regression tests
    - cron: '0 2 * * *'

jobs:
  test:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: [3.9, 3.11]

    steps:
    - uses: actions/checkout@v4

    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v4
      with:
        python-version: ${{ matrix.python-version }}

    - name: Cache pip dependencies
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements*.txt') }}
        restore-keys: |
          ${{ runner.os }}-pip-

    - name: Install system dependencies
      run: |
        sudo apt-get update
        sudo apt-get install -y tesseract-ocr tesseract-ocr-eng

    - name: Install Python dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install pytest pytest-cov pytest-benchmark pytest-mock pytest-xdist
        pip install coverage[toml] codacy-coverage

    - name: Run linting
      run: |
        pip install flake8 black isort
        # Stop the build if there are Python syntax errors or undefined names
        flake8 src/ tests/ --count --select=E9,F63,F7,F82 --show-source --statistics
        # Exit-zero treats all errors as warnings
        flake8 src/ tests/ --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics

    - name: Check code formatting with black
      run: |
        black --check --diff src/ tests/

    - name: Check import sorting with isort
      run: |
        isort --check-only --diff src/ tests/

    - name: Run unit tests
      run: |
        pytest tests/unit/ -v --cov=src --cov-report=xml --cov-report=term-missing --cov-fail-under=95

    - name: Run integration tests
      run: |
        pytest tests/integration/ -v --cov=src --cov-report=xml --cov-report=term-missing --cov-append

    - name: Run performance regression tests
      run: |
        pytest tests/performance/ -v --benchmark-only --benchmark-json=benchmark-results.json
      continue-on-error: true

    - name: Upload coverage reports
      uses: codecov/codecov-action@v3
      with:
        file: ./coverage.xml
        flags: unittests
        name: codecov-umbrella
        fail_ci_if_error: false

    - name: Upload benchmark results
      uses: actions/upload-artifact@v3
      with:
        name: benchmark-results-${{ matrix.python-version }}
        path: benchmark-results.json
      if: always()

    - name: Security scan
      run: |
        pip install safety
        safety check

  performance-baseline:
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main'

    steps:
    - uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install pytest-benchmark

    - name: Run performance baseline
      run: |
        pytest tests/performance/ --benchmark-only --benchmark-save=baseline --benchmark-save-data

    - name: Store performance baseline
      uses: actions/upload-artifact@v3
      with:
        name: performance-baseline
        path: .benchmarks/

  performance-comparison:
    runs-on: ubuntu-latest
    needs: [test, performance-baseline]
    if: github.event_name == 'pull_request'

    steps:
    - uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install pytest-benchmark

    - name: Download baseline
      uses: actions/download-artifact@v3
      with:
        name: performance-baseline
        path: .benchmarks/

    - name: Run performance comparison
      run: |
        pytest tests/performance/ --benchmark-only --benchmark-compare --benchmark-compare-fail=min:5% --benchmark-compare-fail=mean:5%

  deploy:
    runs-on: ubuntu-latest
    needs: [test, performance-comparison]
    if: github.ref == 'refs/heads/main' && github.event_name == 'push'

    steps:
    - uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install build twine

    - name: Build package
      run: python -m build

    - name: Publish to PyPI (if configured)
      run: |
        # Only publish if PYPI_API_TOKEN is set
        if [ -n "$PYPI_API_TOKEN" ]; then
          python -m twine upload dist/*
        else
          echo "PYPI_API_TOKEN not set, skipping PyPI upload"
        fi
      env:
        TWINE_USERNAME: __token__
        TWINE_PASSWORD: ${{ secrets.PYPI_API_TOKEN }}
        PYPI_API_TOKEN: ${{ secrets.PYPI_API_TOKEN }}

  docker-build:
    runs-on: ubuntu-latest
    needs: [test]
    if: github.ref == 'refs/heads/main'

    steps:
    - uses: actions/checkout@v4

    - name: Set up Docker Buildx
      uses: docker/setup-buildx-action@v3

    - name: Log in to Docker Hub
      uses: docker/login-action@v3
      with:
        username: ${{ secrets.DOCKER_USERNAME }}
        password: ${{ secrets.DOCKER_PASSWORD }}

    - name: Build and push Docker image
      uses: docker/build-push-action@v5
      with:
        context: .
        push: true
        tags: |
          ${{ secrets.DOCKER_USERNAME }}/visigate:latest
          ${{ secrets.DOCKER_USERNAME }}/visigate:${{ github.sha }}
        cache-from: type=gha
        cache-to: type=gha,mode=max

  security-scan:
    runs-on: ubuntu-latest
    needs: [test]

    steps:
    - uses: actions/checkout@v4

    - name: Run Trivy vulnerability scanner
      uses: aquasecurity/trivy-action@master
      with:
        scan-type: 'fs'
        scan-ref: '.'
        format: 'sarif'
        output: 'trivy-results.sarif'

    - name: Upload Trivy scan results
      uses: github/codeql-action/upload-sarif@v2
      if: always()
      with:
        sarif_file: 'trivy-results.sarif'

  code-quality:
    runs-on: ubuntu-latest

    steps:
    - uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'

    - name: Install quality tools
      run: |
        python -m pip install --upgrade pip
        pip install radon mccabe flake8 pylint

    - name: Run code quality checks
      run: |
        # Cyclomatic complexity
        radon cc src/ -a -s
        # Maintainability index
        radon mi src/
        # Raw metrics
        radon raw src/

    - name: Generate code quality report
      run: |
        pip install codeclimate-test-reporter
        # Generate coverage and quality reports
        coverage xml
        # This would integrate with code quality services

  notification:
    runs-on: ubuntu-latest
    needs: [test, performance-comparison, deploy, docker-build, security-scan]
    if: always()

    steps:
    - name: Notify on failure
      if: failure()
      run: |
        echo "Pipeline failed - check logs for details"
        # Add notification logic here (Slack, email, etc.)

    - name: Notify on success
      if: success()
      run: |
        echo "Pipeline completed successfully"
        # Add success notification logic here